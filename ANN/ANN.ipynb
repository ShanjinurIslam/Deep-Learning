{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ANN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO3i/BvpH7h71dYPOZcoznL"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Xlnv-apqfnON","colab_type":"text"},"source":["# Artificial Neural Network\n","\n","1.   Model Architecture : Feed Forwarding NN\n","2.   Geometrical Picture\n","3.   Activation Functions\n","4.   Multiclass classification\n","5.   Image Data Representation\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0Y2qt442pvxM","colab_type":"text"},"source":["## Feed Forwarding Neural Network\n","\n","1. Each neuron may calculate something different via different weights\n","\n","2. Neurons are looking for features\n","\n","3. We can continue adding more layers\n","\n","4. Adding more neurons to same layers => Wide\n","   \n","   Adding more layers => Deep\n","\n","5. z is a vector of size (Mx1)<sup>T<sup>\n","  \n","   x is a vector of size (Dx1)<sup>T<sup>\n","\n","   W is a matrix of size DxM\n","\n","   b is a vector of size M\n","\n","   For classification, z = sigmoid( W<sup>T</sup>x + b ) \n","\n","   For regression, z = W<sup>T</sup>x + b \n"]},{"cell_type":"markdown","metadata":{"id":"luxlZVZC0NNJ","colab_type":"text"},"source":["## Multiclass Classification\n","\n","Suppose we calculate the value just before applying the final activation function.  \n","If there are K different classes then output is a vector of size K\n","\n","So there is a requirement of probability for which summation of final vector of size K leads to 1. and each value must be greater than or equal 0\n","\n","We use Activation Function SoftMax for output layer\n","\n"]},{"cell_type":"code","metadata":{"id":"OK0sqZRY2SR0","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}